---
layout: post
title:  general load balance knowledge
category: blog
description: general load balance knowledge
---

## general introduction

### what is Load Balance ?

è´Ÿè½½è®¾å¤‡ä½œä¸ºæœåŠ¡å™¨çš„å‰ç«¯è®¾å¤‡ï¼Œè´Ÿè´£å°†æµé‡æ ¹æ®å„ç§ç­–ç•¥åˆ†å‘åˆ°åç«¯å„ä¸ªçœŸæ­£çš„æœåŠ¡å™¨ä¸Šï¼Œä»è€Œä½¿åç«¯çš„æœåŠ¡å™¨ç¾¤å¾—åˆ°åˆç†æœ‰æ•ˆçš„åº”ç”¨

### why we need Load Balance

ä¿è¯ç½‘ç»œçš„å¯æ‰©å±•æ€§ï¼ŒæœåŠ¡å™¨çš„æ˜“ç®¡ç†æ€§å’Œå¯é æ€§

ç”±äºç›®å‰ç°æœ‰ç½‘ç»œçš„å„ä¸ªæ ¸å¿ƒéƒ¨åˆ†éšç€ä¸šåŠ¡é‡çš„æé«˜ï¼Œè®¿é—®é‡å’Œæ•°æ®æµé‡çš„å¿«é€Ÿå¢é•¿ï¼Œå…¶å¤„ç†èƒ½åŠ›å’Œè®¡ç®—å¼ºåº¦ä¹Ÿç›¸åº”åœ°å¢å¤§ï¼Œä½¿å¾—å•ä¸€çš„æœåŠ¡å™¨è®¾å¤‡æ ¹æœ¬æ— æ³•æ‰¿æ‹…ã€‚åœ¨æ­¤æƒ…å†µä¸‹ï¼Œå¦‚æœæ‰”æ‰ç°æœ‰è®¾å¤‡å»åšå¤§é‡çš„ç¡¬ä»¶å‡çº§ï¼Œè¿™æ ·å°†é€ æˆç°æœ‰èµ„æºçš„æµªè´¹ï¼Œè€Œä¸”å¦‚æœå†é¢ä¸´ä¸‹ä¸€æ¬¡ä¸šåŠ¡é‡çš„æå‡æ—¶ï¼Œè¿™åˆå°†å¯¼è‡´å†ä¸€æ¬¡ç¡¬ä»¶å‡çº§çš„é«˜é¢æˆæœ¬æŠ•å…¥ï¼Œç”šè‡³æ€§èƒ½å†å“è¶Šçš„è®¾å¤‡ä¹Ÿä¸èƒ½æ»¡è¶³å½“å‰ä¸šåŠ¡é‡å¢é•¿çš„éœ€æ±‚.

å°†è´Ÿè½½ï¼ˆå·¥ä½œä»»åŠ¡ï¼‰è¿›è¡Œå¹³è¡¡ã€åˆ†æ‘Šåˆ°å¤šä¸ªæ“ä½œå•å…ƒä¸Šè¿›è¡Œæ‰§è¡Œï¼Œä¾‹å¦‚WebæœåŠ¡å™¨ã€FTPæœåŠ¡å™¨ã€ä¼ä¸šå…³é”®åº”ç”¨æœåŠ¡å™¨å’Œå…¶å®ƒå…³é”®ä»»åŠ¡æœåŠ¡å™¨ç­‰ï¼Œä»è€Œå…±åŒå®Œæˆå·¥ä½œä»»åŠ¡

è´Ÿè½½å‡è¡¡å»ºç«‹åœ¨ç°æœ‰ç½‘ç»œç»“æ„ä¹‹ä¸Šï¼Œå®ƒæä¾›äº†ä¸€ç§å»‰ä»·åˆæœ‰æ•ˆçš„æ–¹æ³•æ‰©å±•ç½‘ç»œè®¾å¤‡å’ŒæœåŠ¡å™¨çš„å¸¦å®½ã€ å¢åŠ ååé‡ã€åŠ å¼ºç½‘ç»œæ•°æ®å¤„ç†èƒ½åŠ›ã€æé«˜ç½‘ç»œçš„çµæ´»æ€§å’Œå¯ç”¨æ€§

### how it work

åœ¨ç¬¬ä¸€ä¸ªåŒ…çš„æ—¶å€™ï¼Œæ ¹æ®ç­–ç•¥é€‰æ‹©åç«¯çš„æœåŠ¡å™¨ï¼Œå»ºç«‹sessionï¼Œä¹Ÿå°±æ˜¯slow pathã€‚åé¢çš„åŒ…é€šè¿‡sessionè¡¨ï¼Œå¾—åˆ°sessionåï¼Œç›´æ¥åˆ©ç”¨ä¸Šæ¬¡çš„ç»“æœï¼Œå°†åŒ…å‘ç»™ç›¸åŒçš„æœåŠ¡å™¨ï¼Œä¹Ÿå°±æ˜¯fast pathã€‚

è´Ÿè½½å‡è¡¡æœ‰ä¸¤æ–¹é¢çš„å«ä¹‰ï¼šé¦–å…ˆï¼Œå¤§é‡çš„å¹¶å‘è®¿é—®æˆ–æ•°æ®æµé‡åˆ†æ‹…åˆ°å¤šå°èŠ‚ç‚¹è®¾å¤‡ä¸Šåˆ†åˆ«å¤„ç†ï¼Œå‡å°‘ç”¨æˆ·ç­‰å¾…å“åº”çš„æ—¶é—´ï¼›å…¶æ¬¡ï¼Œå•ä¸ªé‡è´Ÿè½½çš„è¿ç®—åˆ†æ‹…åˆ°å¤šå°èŠ‚ç‚¹è®¾å¤‡ä¸Šåšå¹¶è¡Œå¤„ç†ï¼Œæ¯ä¸ªèŠ‚ç‚¹è®¾å¤‡å¤„ç†ç»“æŸåï¼Œå°†ç»“æœæ±‡æ€»ï¼Œè¿”å›ç»™ç”¨æˆ·ï¼Œç³»ç»Ÿå¤„ç†èƒ½åŠ›å¾—åˆ°å¤§å¹…åº¦æé«˜

é€šå¸¸ï¼Œè´Ÿè½½ å‡è¡¡ä¼šæ ¹æ®ç½‘ç»œçš„ä¸åŒå±‚æ¬¡ï¼ˆç½‘ç»œä¸ƒå±‚ï¼‰æ¥åˆ’åˆ†ã€‚å…¶ä¸­ï¼Œç¬¬äºŒå±‚çš„è´Ÿè½½å‡è¡¡æŒ‡å°†å¤šæ¡ç‰©ç†é“¾è·¯å½“ä½œä¸€æ¡å•ä¸€çš„èšåˆé€»è¾‘é“¾è·¯ä½¿ç”¨ï¼Œè¿™å°±æ˜¯é“¾è·¯èšåˆ ï¼ˆTrunkingï¼‰æŠ€æœ¯ï¼Œå®ƒä¸æ˜¯ä¸€ç§ç‹¬ç«‹çš„è®¾å¤‡ï¼Œè€Œæ˜¯äº¤æ¢æœºç­‰ç½‘ç»œè®¾å¤‡çš„å¸¸ç”¨æŠ€æœ¯ã€‚

ç°ä»£è´Ÿè½½å‡è¡¡æŠ€æœ¯é€šå¸¸æ“ä½œäºç½‘ç»œçš„ç¬¬å››å±‚æˆ–ç¬¬ä¸ƒå±‚ï¼Œè¿™æ˜¯é’ˆå¯¹ç½‘ç»œåº”ç”¨ çš„è´Ÿè½½å‡è¡¡æŠ€æœ¯ï¼Œå®ƒå®Œå…¨è„±ç¦»äºäº¤æ¢æœºã€æœåŠ¡å™¨è€Œæˆä¸ºç‹¬ç«‹çš„æŠ€æœ¯è®¾å¤‡ã€‚è¿™ä¹Ÿæ˜¯æˆ‘ä»¬ç°åœ¨è¦è®¨è®ºçš„å¯¹è±¡ã€‚

### how it select

* ä¾åºRound Robin ã€€
* æ¯”é‡Weighted Round Robin
* æµé‡æ¯”ä¾‹Traffic
* ä½¿ç”¨è€…ç«¯User ã€€
* åº”ç”¨åˆ«Application
* è”æœºæ•°é‡Session
* æœåŠ¡åˆ«Service
* è‡ªåŠ¨åˆ†é…Auto Mode

### deployment

è´Ÿè½½å‡è¡¡æœ‰ä¸‰ç§éƒ¨ç½²æ–¹å¼ï¼šè·¯ç”±æ¨¡å¼ã€æ¡¥æ¥æ¨¡å¼ã€æœåŠ¡ç›´æ¥è¿”å›æ¨¡å¼ã€‚

è·¯ç”±æ¨¡å¼
æœåŠ¡å™¨çš„ç½‘å…³å¿…é¡»è®¾ç½®æˆè´Ÿè½½å‡è¡¡æœºçš„LANå£åœ°å€ï¼Œä¸”ä¸WANå£åˆ†ç½²ä¸åŒçš„é€»è¾‘ç½‘ç»œã€‚å› æ­¤æ‰€æœ‰è¿”å›çš„æµé‡ä¹Ÿéƒ½ç»è¿‡è´Ÿè½½å‡è¡¡ã€‚è¿™ç§æ–¹å¼å¯¹ç½‘ç»œçš„æ”¹åŠ¨å°ï¼Œèƒ½å‡è¡¡ä»»ä½•ä¸‹è¡Œæµé‡ã€‚

æ¡¥æ¥æ¨¡å¼
æ¡¥æ¥æ¨¡å¼é…ç½®ç®€å•ï¼Œä¸æ”¹å˜ç°æœ‰ç½‘ç»œã€‚è´Ÿè½½å‡è¡¡çš„WANå£å’ŒLANå£åˆ†åˆ«è¿æ¥ä¸Šè¡Œè®¾å¤‡å’Œä¸‹è¡ŒæœåŠ¡å™¨ã€‚LANå£ä¸éœ€è¦é…ç½®IPï¼ˆWANå£ä¸LANå£æ˜¯æ¡¥è¿æ¥ï¼‰ï¼Œæ‰€æœ‰çš„æœåŠ¡å™¨ä¸è´Ÿè½½å‡è¡¡å‡åœ¨åŒä¸€é€»è¾‘ç½‘ç»œä¸­

æœåŠ¡ç›´æ¥è¿”å›æ¨¡å¼
è¿™ç§å®‰è£…æ–¹å¼è´Ÿè½½å‡è¡¡çš„LANå£ä¸ä½¿ç”¨ï¼ŒWANå£ä¸æœåŠ¡å™¨åœ¨åŒä¸€ä¸ªç½‘ç»œä¸­ï¼Œäº’è”ç½‘çš„å®¢æˆ·ç«¯è®¿é—®è´Ÿè½½å‡è¡¡çš„è™šIPï¼ˆVIPï¼‰ï¼Œè™šIPå¯¹åº”è´Ÿè½½å‡è¡¡æœºçš„WANå£ï¼Œ
è½½å‡è¡¡æ ¹æ®ç­–ç•¥å°†æµé‡åˆ†å‘åˆ°æœåŠ¡å™¨ä¸Šï¼ŒæœåŠ¡å™¨ç›´æ¥å“åº”å®¢æˆ·ç«¯çš„è¯·æ±‚ã€‚å› æ­¤å¯¹äºå®¢æˆ·ç«¯è€Œè¨€ï¼Œå“åº”ä»–çš„IPä¸æ˜¯è´Ÿè½½å‡è¡¡æœºçš„è™šIPï¼ˆVIPï¼‰ï¼Œè€Œæ˜¯æœåŠ¡å™¨è‡ªèº«çš„IPåœ°å€ã€‚ä¹Ÿå°±æ˜¯è¯´è¿”å›çš„æµé‡æ˜¯ä¸ç»è¿‡è´Ÿè½½å‡è¡¡çš„ã€‚å› æ­¤è¿™ç§æ–¹å¼é€‚ç”¨å¤§æµé‡é«˜å¸¦å®½è¦æ±‚çš„æœåŠ¡ã€‚


## Ananta: Cloud Scale Load Balancing

contributions:  

 * Identifying the requirements and design space for a cloudscale solution for layer-4 load balancing.
 * Providing design, implementation and evaluation of Ananta that combines techniques in networking and distributed systems
   to refactor load balancer functionality in a novel way to meet scale, performance and reliability requirements.
 * Providing measurements and insights from running Anant in a large operational Cloud.

Ananta, a scale-out layer-4 load balancer that runs on commodity hardware and
meets the performance, reliability and operational requirements of multi-tenant cloud computing environments

Ananta combines existing techniques in routing and distributed systems in a unique way and
splits the components of a load balancer into a consensus-based reliable control plane and a decentralized scale-out data plane

an agent in every host that can take over the packet modification function.
direct server return (DSR) and network address translation (NAT) capabilities across layer-2 boundaries.

<img src="/images/LB/Ananta/data-plane-tiers.png" alt="sdn" width="1000"></a>

at the topmost/second/third tiers:

  * routers provide load distribution at the network layer (layer-3) based on the Equal Cost Multi Path (ECMP) routing protocol
  * a scalable set of dedicated servers for load balancing, called multiplexers (Mux), maintain connection flow state in memory
  and do layer-4 load distribution to application servers
  * virtual switch on every server provides stateful NAT functionality.

no outbound trafffic has to pass through the Mux  and ability to offload multiplexer functionality down to the host.

The SDN controller

maintains high availability via state replication based on the Paxos [14] distributed consensus protocol.
The controller also implements real-time port allocation for outbound NAT, also known as Source NAT (SNAT)

keeping per-connection state is necessary to maintain application uptime due to the dynamic nature of the cloud.
weighted random load balancing policy, which reduces the need for per-flow state synchronization among load balancer instances.


Background:

 * Each machine is assigned a private Direct IP (DIP) address
 * service is assigned one public Virtual IP (VIP) address
 * A service exposes zero or more external endpoints that each receive inbound traffic on a specific protocol and port on the VIP
 * All outbound traffic originating from a service is Source NATâ€™ed (SNAT) using the same VIP address as well

Designï¼š

 * assumptionsï¼š load balancing policies that require global knowledgeï¼Œrandomly distributing connections across servers
based on their weights (The weights are derived based on the size of the VM or other capacity metrics)
 * offloads significant data plane and control plane functionality down to the hypervisor in end systems. The hypervisor
needs to handle state only for the VMs hosted on it.

### Architecture:

Ananta is a loosely coupled distributed system comprising three main components:  

 * Ananta Manager (AM),
 * Multiplexer (Mux) and
 * Host Agent (HA).

<img src="/images/LB/Ananta/Arch.png" alt="sdn" width="1000"></a>

we first discuss the load balancer configuration and the overall packet flow

<img src="/images/LB/Ananta/in-conn.png" alt="sdn" width="1000"></a>

 * routers to distribute packets destined for the VIP across all
the Mux nodes based on ECMP
 * the Mux chooses a DIP for the connection based on its load balancing algorithm, It then encapsulates the received packet using IP-in-IP protocol
  setting the selected DIP as the destination address in the outer header
 * sends it out using regular IP routing at the Mux
 * The HA, located on the same physical machine as the target DIP, intercepts this encapsulated packet,
   removes the outer header, and rewrites the destination address and port.
 * remembers this NAT state. The HA then sends the rewritten packet to the VM
 * When the VM sends a reply packet for this connection, it is intercepted by the HA
 * The HA does a reverse NAT based on the state and rewrites the source address and port
    It then sends the packet out to the router towards the source of this connection

Note:  
Not all packets of a connection would end up at the same Mux, however all packets for a single
connection must be delivered to the same DIP.   
Muxes achieve this via a combination of consistent hashing and state management

<img src="/images/LB/Ananta/out-conn.png" alt="sdn" width="1000"></a>

 * A VM sends a packet containing its DIP as the source address, portd as the port and an external address as
the destination address.
 * The HA intercepts this packet and recognizes that this packet needs SNAT. It then holds the packet
in a queue and sends a message to AM requesting an externally routable VIP and a port for this connection
 * AM allocates a (V IP; ports) from a pool of available ports and configures each Mux with this allocation.
 * AM then sends this allocation to the HA.
 * The HA uses this allocation to rewrite the packet so that its source address and port are now (V IP; ports).
   The HA sends this rewritten packet directly to the router. The return packets from the external destination are handled similar to inbound connections.
   The return packet is sent by the router to one of the Mux nodes.
 * The Mux already knows that DIP2 should receive this packet , so it encapsulates the packet with DIP2 as the destination and sends it out
 * The HA intercepts the return packet, performs a reverse translation so that the packetâ€™s destination address and port are now
   (DIP; portd). The HA sends this packet to the VM.

   <img src="/images/LB/Ananta/fast-conn.png" alt="sdn" width="1000"></a>


### Mux

Route Management  
Each Mux is a BGP speaker. It starts advertising a route for that VIP to its firsthop router with itself as the next hop.
Running the BGP protocol on the Mux provides automatic failure detection and recovery.

Packet Handling  

a mapping table (VIP map )  

  * a VIP endpoint, i.e., three-tuple (VIP, IP protocol, port), to a list of DIPs
  * is computed by AM and sent to all the Muxes in a Mux Pool

it computes a hash and  uses this hash to lookup a DIP from the list of DIPs in the associated map.
it encapsulates packet with an outer IP header and forwards this packet to the DIP.

Note:  
All Muxes in a Mux Pool use the exact same hash function and seed value.
Since all Muxes have the same mapping table,
it doesnâ€™t matter which Mux a given new connection goes to, it will be directed to the same DIP

Stateful entries are used for load balancing and stateless entries are used for SNAT.
Every non-SYN TCP packet, and every packet for connection-less protocols, is matched against this flow table first,
and if a match is found it is forwarded to the DIP from the flow table. This ensures that once a connection is directed to a DIP


## Maglev: A Fast and Reliable Software Network Load Balancer

contributions are:   
1) present the design and implementation of Maglev,   
2) share experiences of operating Maglev at a global scale,   
3) demonstrate the capability of Maglev through extensive evaluations.

<img src="/images/LB/Maglev/gen.png" alt="sdn" width="1000"></a>

Maglev is Googleâ€™s network load balancer. It is a large distributed software system that runs on commodity Linux servers.

Network routers distribute packets evenly to the Maglev machines via Equal Cost Multipath (ECMP);
each Maglev machine then matches the packets to their corresponding services and spreads them evenly to the service endpoints.

Maglev is optimized for packet processing performance and be also equipped with consistent hashing and connection tracking features.

the design and implementation are highly complex and challenging:  

 * each individual machine in the system must provide high throughput.
 * provide connection persistence: packets belonging to the same connection should always be directed to the same service endpoint

### System Overview

<img src="/images/LB/Maglev/packet-flow.png" alt="sdn" width="1000"></a>

Every Google service has one or more Virtual IP ad-dresses (VIPs). A VIP served by `multiple service endpoints` behind Maglev.
Maglev associates each VIP with a set of service endpoints and announces it to the router over BGP.

The setup for large clusters is more complicated to be scaling, so hardware encapsulators are deployed behind
the router, which tunnel packets from routers to Maglev machines.

#### config

<img src="/images/LB/Maglev/config.png" alt="sdn" width="1000"></a>

each Maglev machine contains a controller and a forwarder. it earn the VIPs to be served from configuration objects.
the controller periodically checks the health status of the forwarder and decides whether to announce or withdraw all the VIPs via BGP.

At the forwarder, each VIP is configured with one or more backend pools.
Each backend pool is associated with one or more health checking methods by IP.

### Forwarder

<img src="/images/LB/Maglev/forward.png" alt="sdn" width="1000"></a>

The forwarder receives packets from the NIC, rewrites them with proper GRE/IP headers and then sends them back to the NIC.

After Packets received by the NIC, steering module process it, it calculates the `5-tuple hash1`  and assigns them to different `receiving queues` based on hash value(Each receiving queue is attached to a packet rewriter thread).

The packet thread first tries to match each packet to a `configured VIP`. Then it recomputes the `5-tuple hash` and
looks up the hash value in the `connection tracking table`. (The connection table stores backend selection results for recent connections)

If a match is found and the selected backend is still healthy, the result is simply reused.
Otherwise the thread consults the `consistent hashing module` and selects a new backend;
it also adds an entry to the connection table for future packets with the same 5-tuple.

The forwarder maintains one connection table per packet thread to avoid access contention.

After a backend is selected, the packet thread encapsulates the packetwith proper GRE/IP headers
and sends it to the attached transmission queue. The muxing module then polls all transmission queues and
passes the packets to the NIC.

#### Fast Packet Processing

<img src="/images/LB/Maglev/fast.png" alt="sdn" width="1000"></a>

Maglev bypass the kernel entirely for packet processing.

When Maglev is started, it pre-allocates a packet pool that is shared between the NIC and the forwarder.
Both the steering and muxing modules maintain a ring queue of pointers pointing to packets in the packet pool.

### backend selection

it need send all packets of a connection to the same backend:  

  * First, we select a backend using a new form of consistent hashing which distributes traffic very evenly.
  * Then we record the selection in a local connection tracking table.

connection tracking table:  
 uses a fixed-size hash table mapping 5-tuple hash values to backends.

### Consistent Hashing

how about share connection state among all Maglev machines ?  
  it may negatively affect forwarding performance

A better-performing solution is to use local consistent hashing, The idea is to generate a large lookup table with each
backend taking a number of entries in the table  

 * load balancing: each backend will receive an almost equal number of connections.
 * minimal disruption: when the set of backends changes, a connectionwill likely be sent to the same backend as it was before.

 Maglev hashing


### Fragment Handling

assumptions:

* First, all fragments of the same datagram must be received by the same Maglev.
* Second, theMaglev must make consistent backend selection decisions for unfragmented packets, first fragments, and non-first fragments.

but some routers use 5-tuple hashing for first fragments and 3-tuple for non-first fragments.  
so Each Maglev is configured with a special backend pool consisting of all Maglevs within the cluster.
Upon receipt of a fragment, Maglev computes its 3-tuple hash using the L3 header and forwards it to a Maglev from the pool based on thehash value.  
Since all fragments belonging to the same datagram contain the same 3-tuple, they are guaranteed to be redirected to the same Maglev. We use the GRE
recursion control field to ensure that fragments are only redirected once


## Reference link

[network guide](https://www.rdoproject.org/networking/networking-in-too-much-detail/)
